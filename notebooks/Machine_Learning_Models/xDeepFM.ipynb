{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b5a9b51-859f-47aa-a111-f1f9430e6341",
   "metadata": {},
   "source": [
    "- xDeepFM(xTreeme)\n",
    "\n",
    "- DeepFMを拡張して，クロスネット(CIN: Compressed Interaction Network)を用いて，高階の特徴交差を自動的に学習するモデルのこと．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e444d5-889d-471a-a057-7c4bb05b9eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "         Input\n",
    "           |\n",
    "   ┌───────┴────────┐\n",
    "   │                │\n",
    " Embedding       Linear（wide）\n",
    "   │                │\n",
    "   └───────┬────────┘\n",
    "           │\n",
    "   ┌───────┴──────────────┐\n",
    "   │                      │\n",
    " Deep part (MLP)     CIN（Cross）\n",
    "   │                      │\n",
    "   └────────┬─────────────┘\n",
    "            │\n",
    "         Concatenate\n",
    "            │\n",
    "          Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9232f24-37d1-4387-9c4c-9fd787951c1b",
   "metadata": {},
   "source": [
    "- 下の三つのパートを統合したハイブリッドモデル．\n",
    "\n",
    "| パート名       | 役割                                           |\n",
    "| ---------- | -------------------------------------------- |\n",
    "| **Linear** | 明示的な1次特徴（Wide部分と同じ）                          |\n",
    "| **CIN**    | 高階特徴交差を自動で学習（Compressed Interaction Network） |\n",
    "| **Deep**   | 埋め込みベクトルをMLPに通して非線形な関係を学習                    |\n",
    "\n",
    "- 損失関数は，sigmoid関数を用いる．\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41cfea70-7a75-478f-95b5-0762751b3465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # 0:all, 1:filter INFO, 2:filter WARNING, 3:only ERROR\n",
    "\n",
    "# データ読み込み\n",
    "cols = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "df = pd.read_csv('u.data', sep='\\t', names=cols)\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "# Leave-One-Out分割\n",
    "df['rank'] = df.groupby('user_id')['timestamp'].rank(method='first', ascending=False)\n",
    "train_df = df[df['rank'] > 1].copy()\n",
    "test_df = df[df['rank'] == 1].copy()\n",
    "\n",
    "# IDを数値に変換\n",
    "user_enc = LabelEncoder()\n",
    "item_enc = LabelEncoder()\n",
    "train_df['user'] = user_enc.fit_transform(train_df['user_id'])\n",
    "train_df['item'] = item_enc.fit_transform(train_df['item_id'])\n",
    "test_df = test_df[test_df['user_id'].isin(user_enc.classes_)]\n",
    "test_df = test_df[test_df['item_id'].isin(item_enc.classes_)]\n",
    "test_df['user'] = user_enc.transform(test_df['user_id'])\n",
    "test_df['item'] = item_enc.transform(test_df['item_id'])\n",
    "\n",
    "# ラベル（rating >= 4を正例）\n",
    "train_df['label'] = (train_df['rating'] >= 4).astype(int)\n",
    "test_df['label'] = (test_df['rating'] >= 4).astype(int)\n",
    "\n",
    "num_users = train_df['user'].nunique()\n",
    "num_items = train_df['item'].nunique()\n",
    "\n",
    "# Dataset定義\n",
    "class FM_Dataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.users = torch.LongTensor(df['user'].values)\n",
    "        self.items = torch.LongTensor(df['item'].values)\n",
    "        self.labels = torch.FloatTensor(df['label'].values)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.users[idx], self.items[idx], self.labels[idx]\n",
    "\n",
    "train_loader = DataLoader(FM_Dataset(train_df), batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(FM_Dataset(test_df), batch_size=256)\n",
    "\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b18501a3-97b7-4fb7-8486-45beeee65555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CIN(nn.Module):\n",
    "    def __init__(self, input_dim, cin_layers):\n",
    "        super(CIN, self).__init__()\n",
    "        self.cin_layers = nn.ModuleList()\n",
    "        self.field_nums = [input_dim]\n",
    "        self.embedding_dim = None  # 初期化時には不明\n",
    "\n",
    "        for layer_size in cin_layers:\n",
    "            in_channels = self.field_nums[-1] * self.field_nums[0]  # 現在層 × 初期層\n",
    "            self.cin_layers.append(\n",
    "                nn.Conv1d(in_channels=in_channels, out_channels=layer_size, kernel_size=1)\n",
    "            )\n",
    "            self.field_nums.append(layer_size)\n",
    "\n",
    "    def forward(self, x_0):\n",
    "        # x_0: (B, F, D)\n",
    "        if self.embedding_dim is None:\n",
    "            self.embedding_dim = x_0.size(2)\n",
    "\n",
    "        xs = []\n",
    "        x = x_0\n",
    "        for i, conv in enumerate(self.cin_layers):\n",
    "            B, H_k, D = x.size()\n",
    "            x_i = torch.einsum(\"bhd,bmd->bhm\", x, x_0)  # (B, H_k, H_0)\n",
    "            x_i = x_i.view(B, H_k * x_0.size(1), 1)     # (B, H_k * H_0, 1)\n",
    "            x_i = conv(x_i)                             # (B, H_{k+1}, 1)\n",
    "            x_i = F.relu(x_i)\n",
    "            x_i = x_i.view(B, -1, self.embedding_dim)   # (B, H_{k+1}, D)\n",
    "            xs.append(x_i)\n",
    "            x = x_i\n",
    "\n",
    "        x_stack = torch.cat(xs, dim=1)  # (B, sum(H_l), D)\n",
    "        return torch.sum(x_stack, dim=2)  # (B, sum(H_l))\n",
    "\n",
    "\n",
    "\n",
    "class xDeepFM(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embedding_dim=16, cin_layers=[16, 16], mlp_layers=[32, 16]):\n",
    "        super(xDeepFM, self).__init__()\n",
    "        self.user_embed = nn.Embedding(num_users, embedding_dim)\n",
    "        self.item_embed = nn.Embedding(num_items, embedding_dim)\n",
    "\n",
    "        self.linear_user = nn.Embedding(num_users, 1)\n",
    "        self.linear_item = nn.Embedding(num_items, 1)\n",
    "\n",
    "        self.cin = CIN(input_dim=2, cin_layers=cin_layers)\n",
    "\n",
    "        input_dim = 2 * embedding_dim\n",
    "        mlp = []\n",
    "        for h in mlp_layers:\n",
    "            mlp.append(nn.Linear(input_dim, h))\n",
    "            mlp.append(nn.ReLU())\n",
    "            input_dim = h\n",
    "        self.mlp = nn.Sequential(*mlp)\n",
    "\n",
    "        final_dim = 1 + 1 + cin_layers[-1] + mlp_layers[-1]\n",
    "        self.output = nn.Linear(final_dim, 1)\n",
    "\n",
    "    def forward(self, user, item):\n",
    "        u_emb = self.user_embed(user)  # (B, D)\n",
    "        i_emb = self.item_embed(item)  # (B, D)\n",
    "\n",
    "        linear_part = self.linear_user(user) + self.linear_item(item)\n",
    "\n",
    "        cin_input = torch.stack([u_emb, i_emb], dim=1)  # (B, F=2, D)\n",
    "        cin_out = self.cin(cin_input)\n",
    "\n",
    "        deep_input = torch.cat([u_emb, i_emb], dim=1)\n",
    "        deep_out = self.mlp(deep_input)\n",
    "\n",
    "        all_concat = torch.cat([linear_part, torch.sum(cin_out, dim=1, keepdim=True), deep_out], dim=1)\n",
    "        out = torch.sigmoid(self.output(all_concat)).squeeze(1)\n",
    "        return out\n",
    "\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "476b11e3-08af-41d4-a153-6c8d0050597d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [16, 32, 1], expected input[128, 2, 1] to have 32 channels, but got 2 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m users, items, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     16\u001b[0m     users, items, labels \u001b[38;5;241m=\u001b[39m users\u001b[38;5;241m.\u001b[39mto(device), items\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 17\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43musers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitems\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(preds, labels)\n\u001b[1;32m     19\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/.pyenv/versions/py310-surprise/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/py310-surprise/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[10], line 69\u001b[0m, in \u001b[0;36mxDeepFM.forward\u001b[0;34m(self, user, item)\u001b[0m\n\u001b[1;32m     66\u001b[0m linear_part \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_user(user) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_item(item)\n\u001b[1;32m     68\u001b[0m cin_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([u_emb, i_emb], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (B, F=2, D)\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m cin_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcin_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m deep_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([u_emb, i_emb], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     72\u001b[0m deep_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(deep_input)\n",
      "File \u001b[0;32m~/.pyenv/versions/py310-surprise/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/py310-surprise/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[10], line 29\u001b[0m, in \u001b[0;36mCIN.forward\u001b[0;34m(self, x_0)\u001b[0m\n\u001b[1;32m     27\u001b[0m x_i \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbhd,bmd->bhm\u001b[39m\u001b[38;5;124m\"\u001b[39m, x, x_0)  \u001b[38;5;66;03m# (B, H_k, H_0)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m x_i \u001b[38;5;241m=\u001b[39m x_i\u001b[38;5;241m.\u001b[39mview(B, H_k \u001b[38;5;241m*\u001b[39m x_0\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m), \u001b[38;5;241m1\u001b[39m)     \u001b[38;5;66;03m# (B, H_k * H_0, 1)\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m x_i \u001b[38;5;241m=\u001b[39m \u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_i\u001b[49m\u001b[43m)\u001b[49m                             \u001b[38;5;66;03m# (B, H_{k+1}, 1)\u001b[39;00m\n\u001b[1;32m     30\u001b[0m x_i \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x_i)\n\u001b[1;32m     31\u001b[0m x_i \u001b[38;5;241m=\u001b[39m x_i\u001b[38;5;241m.\u001b[39mview(B, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_dim)   \u001b[38;5;66;03m# (B, H_{k+1}, D)\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/py310-surprise/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/py310-surprise/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.pyenv/versions/py310-surprise/lib/python3.10/site-packages/torch/nn/modules/conv.py:375\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/py310-surprise/lib/python3.10/site-packages/torch/nn/modules/conv.py:370\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(\n\u001b[1;32m    360\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    361\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    369\u001b[0m     )\n\u001b[0;32m--> 370\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [16, 32, 1], expected input[128, 2, 1] to have 32 channels, but got 2 channels instead"
     ]
    }
   ],
   "source": [
    "# 学習・評価\n",
    "\n",
    "from torch.optim import Adam\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = xDeepFM(num_users, num_items).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 学習ループ\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for users, items, labels in train_loader:\n",
    "        users, items, labels = users.to(device), items.to(device), labels.to(device)\n",
    "        preds = model(users, items)\n",
    "        loss = criterion(preds, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}: Loss = {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5c6229-586a-42cb-b8bc-afea7369d188",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ac0d19-b2c9-4bfa-9490-73f607ac49f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py310-surprise)",
   "language": "python",
   "name": "py310-surprise"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
